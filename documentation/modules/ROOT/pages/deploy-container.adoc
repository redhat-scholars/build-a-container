include::_attributes.adoc[]

:OCP_PROJECT: {USER}

= Deploying to OpenShift
:navtitle: Deploying to OpenShift
:description: Introduction to Containers, Container Security and Container Tools including Podman and Buildah
:keywords: docker, podman, buildah, skopeo, security, secure containers, containers, Red Hat, RHEL, Linux, Containerization, cloud, build a container, workshop, cloud native, openshift

_15 MINUTE EXERCISE_

Now that we have a container let's demonstrate how one might deploy that container to a platform like Red Hat's Enterprise Kubernetes solution, OpenShift.

[#openshift_login]
== Login to OpenShift

First, let's log into your OpenShift cluster.  Choose the tab that is most appropriate to your current lab.  If you are unsure, ask your lab techs.

[tabs]
====
Provisioned::
+
--
. You should be able to reach the OpenShift cluster provisioned for this lab link:https://console-openshift-console.apps.{CLUSTER_SUBDOMAIN}/[here^]
. You should be met with a login challenge screen
+
image::openshift-login-challenge.jpg[]
+
. Enter the following details
** *Username*: {USER}
** *Password*: openshift
 
--
OpenShift Developer Sandox::
+
--

.PREREQUISITES
****
This section assumes you:

* Already have a Red Hat account
* Requested and have had an OpenShift Developer Sandbox approved prior to attempting this section
****

. Navigate to the link:https://red.ht/dev-sandbox[Red Hat Developer OpenShift Developer Sandbox page^]
. Click on "start your sandbox" and enter your Red Hat account login details
. From the OpenShift sandbox page, log into your sandbox by clicking on the "DevSandbox" button on the login challenge page
+
image::dev-sandbox-login-challenge.png[]
+
. Once you login, you should see your `-dev` project in the Developer Perspective
+
image::dev-sandbox-dev-perspective.jpg[]
--
====

If you've logged in successfully, you should find yourself on the (empty) start page for the Developer Perspective one the `{OCP_PROJECT}` projectfootnote:[_project_ is an OpenShift specific term.  For the purposes of this lab you can think of is as synonymous with the Kubernetes concept of a _namespace]

image::ocp-developer-perspective.jpg[]

=== Console Login

For this section we're going to want to issue commands to OpenShift from the command line.  OpenShift has a CLI called `oc` footnote:[`oc` is built on top of `kubectl`, the generic Kubernetes CLI.  Any command you can issue with `kubectl` you can issue with `oc`, but `oc` builds upon `kubectl` with OpenShift specific commands, such as `oc login`] which we will leverage.

Here again, choose the proper tab for your setup

[tabs]
====
Provisioned::
+
--
. From the (CodeServer) terminal, enter the following command to log into OpenShift
+
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
oc login https://api.{CLUSTER_SUBDOMAIN}:6443 \#<.>
    --username {USER} \
    --password 'openshift'
----
<.> This is the URL of the REST API for OpenShift with which the CLI interacts
+
.Insecure connections
****
If you are met the with following question in the console

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
The server uses a certificate signed by an unknown authority.
You can bypass the certificate check, but any data you send to the server could be intercepted by others.
Use insecure connections? (y/n): 
----

You can safely answer `y` (yes) at the prompt
****
--
OpenShift Developer Sandbox::
+
--
. From your OpenShift Console UI, click the dropdown in the upper right (with your account name) and select `Copy login command`
+
image::copy-login-command.png[]
+
. Next, when presented the login challenge select `DevSandbox`
+
image::dev-sandbox-login-challenge.png[]
+
. Click the "Display Token" link
. Copy the string as indicated in the image
+
image::copy-login-token.jpg[]
+
. Paste the command into a Code Server terminal
--
====

If you have logged on successfully, running the following command: 

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
oc whoami
----

Should yield your OpenShift username (below represents username shown if using Provisioned Cluster)

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
{USER}
----

[#image_registry]
== Upload Image to Image Registry

Let's go back to the container that we finished link:build-your-own-container-containerfile.html?{guide-query-string}#finished_container[here^] and look at what a simple deployment to the `{OCP_PROJECT}` project.

=== OpenShift and Image Registries

In order to be able to run an image in OpenShift or Kubernetes we need to put our image somewhere we're OpenShift can find it.  This usually involves uplaoding the image to either a public or private container registry.  Public registries include Red Hat's link:quay.io[quay.io^] and Docker's link:https://hub.docker.com/[Docker Hub^]

One of the features that OpenShift adds to Kubernetes is an in-built container registry called an `ImageStream`.  We're going to create an `ImageStream` to upload our container to where OpenShift can find it.

We can create the image stream either from the OpenShift Console (UI) or from the terminal (command line).  

. Enter the following command
+
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
oc create imagestream \#<.>
    -n {OCP_PROJECT} \#<.>
    {ocp_secure_image_name} #<.>
----
<.> `ImageStream` is an OpenShift specific Kubernetes resource that represents a project specific container registry
<.> This is the namespace the `ImageStream` should be bound to
<.> This is the name of the image registry we want to create
+
[.console-output]
[source,bash,subs="+macros,+attributes"]
----
imagestream.image.openshift.io/my-secure-web-server-image created
----
+
. With our `ImageStream` created, we can find our registry endpoint with this command
+
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
OCP_REGISTRY_URL=$(oc get imagestream {ocp_secure_imagestream_name} \
    -n {USER} \
    -o jsonpath='{.status.publicDockerImageRepository}') #<.>
----
<.> `-o` is used to specify the output type.  In this case we specify `jsonpath` which means give the output as JSON and then act as if it were piped to `jq -r` meaning we specify the field in the JSON we are looking for
+
. Once we have the `publicDockerImageRepository` we can use podman to login into it with our OpenShift credentials
+
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
podman login \
    pass:[${OCP_REGISTRY_URL}] \
    --username {USER} \
    --password "$(oc whoami -t)" #<.>
----
<.> You must log into imagestream registries using a token and not your user's password.  `oc whoami -t` returns the currently active token for a given OpenShift session
+
. This should yield the following output, which indicates that you've authenticated with the `ImageStream` internal registry
+
[.console-output]
[source,bash,subs="+macros,+attributes"]
----
Login Succeeded!
----
+
. Now we're going to use a new `podman` command called `tag` to associate our local image with an image that could exist in our ImageStream registry
+
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
podman tag \
    {apache-server-containerfile-image} \#<.>
    pass:[${OCP_REGISTRY_URL}]:latest #<.>
----
<.> This, remember is the local name of the image we created xref::build-your-own-container.adoc#finished_container[in the previous section,window=_blank]
<.> If you `echo` this in a shell, this will look something like `default-route-openshift-image-registry.apps.{CLUSTER_SUBDOMAIN}/{USER}/pass:[my-secure-web-server-image:latest]` which is similar to container identification names as we discussed in the xref:container-anatomy.adoc#container_identification[Container Anatomy section, window=_blank]
+ 
. Once tagged, we should now be able to push this image into the ImageStream using the `podman push` command 
+
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
podman push pass:[${OCP_REGISTRY_URL}]:latest
----
+
. You should see output similar to the following
+
[.console-output]
[source,bash,subs="+macros,+attributes"]
----
Getting image source signatures
Copying blob 1d505cb9245a done  
Copying blob 9e12a51e507a done  
Copying blob 01d2fb866535 done  
Copying config f8b584bce6 done  
Writing manifest to image destination
Storing signatures
----

Now we have our image in a place where we can refer to it

[#simple_container_run]
== Run Container Image on OpenShift

The simplest way to get a container image up and running in OpenShift is with the `oc run` command.  This will create what's called a `pod` to house our container that runs based on the image definition we just uploaded to the ImageStream

. One of the benefits of using ImageStreams is that the cluster internal address of the ImageStream repo does not require authentication.  Let's use that for the image location for running our pod
+
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
CLUSTER_LOCAL_REGISTRY_URL=$(oc get imagestream \
    {ocp_secure_imagestream_name} \#<.>
    -o jsonpath='{.status.dockerImageRepository}') #<.>
----
<.> Notice that this is the same ImageStream name we've been using
<.> This time we're looking not for the `publicDockerImageRepository` from the ImageStream definition, but the `dockerImageRepository` which is the same as the cluster local address of the repo
+
. Run the following command from the Code Server terminal
+
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
oc run \
    my-web-server \#<.>
    -n {OCP_PROJECT} \
    --image pass:[${CLUSTER_LOCAL_REGISTRY_URL}]:latest
----
<.> The name that will be given to the pod
+
[.console-output]
[source,bash,subs="+macros,+attributes"]
----
pod/my-web-server created
----
+
. Switch to the OpenShift Console (UI) and look at the link:https://console-openshift-console.apps.{CLUSTER_SUBDOMAIN}/topology/ns/{OCP_PROJECT}?view=graph[developer perspective for your project^].  You should see the pod running
+
.Our image running in OpenShift
image::ocp-running-secure-image.jpg[]

[#expose_container]
== Exposing Container Website

Our container is running in OpenShift, but we'd like the rest of the world to have access to our guestbook.  To do this on OpenShift we create a `Service` that "exposes" our pod and then a `Route` that "exposes" our service to the world outside the OpenShift cluster.

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
oc expose \#<.>
    --port 8080 \#<.>
    pod/my-web-server #<.>
----
<.> `expose` is an `oc` CLI specific command
<.> We want our service to expose port 8080
<.> Many resources can be exposed.  Exposing a pod (as we're referencing here) creates a service that points to the pod based on the svcs the pod exports

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
service/my-web-server exposed
----

Once our service is created, we can expose our `Service` (or `svc`)

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
oc expose \
    svc/my-web-server
----

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
route.route.openshift.io/my-web-server exposed
----

Exposing our service creates a `Route` in OpenShift.  We can interrogate this created route to determine what the publicly accessible endpoint is for our service:

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
echo "http://$(oc get route my-web-server -o jsonpath='{.spec.host}')/hello.html"
----

// FIXME: Add tab or instructions for doing this via Topology View in OpenShift UI

This should return output something like this:

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
pass:[http://my-web-server-]{USER}pass:[.apps.]{CLUSTER_SUBDOMAIN}/hello.html
----

Copy the output and paste it into any internet connected browser.  You should get access to an operating guestbook

image::sandbox-website.png[]

[#openshift_security]
== OpenShift Security

In the link:build-your-own-container-containerfile.html?{guide-query-string}#basic_containerfile[building secure container section^] we spent a lot of time making our image suitable for running on OpenShift.  

Let's take a quick look at how OpenShift adds a layer of protection around our running of images by looking at what happens when we try to run our insecure container on it.

. First let's make use of OpenShift's internal image registry again (the `ImageStream` we created before) by tagging the insecure image with a name that binds it to to that registry
+
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
podman tag \
    {apache-server-image-insecure} \
    pass:[${OCP_REGISTRY_URL}]:insecure #<.>
----
<.> Same exact `ImageStream` image registry, just different tag
+
. With the image tagged for the `ImageStream` registry, we can now push it
+
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
podman push \
    pass:[${OCP_REGISTRY_URL}]:insecure
----
+
[.console-output]
[source,bash,subs="+macros,+attributes"]
----
Getting image source signatures
Copying blob bcf3865bf7f7 done  
Copying blob 86284899b0cc done  
Copying blob c9e02f9d3afe done  
Copying blob 4e7c990a129f done  
Copying blob 210af8709b71 done  
Copying blob 123257361dae done  
Copying blob 47e96512450e done  
Copying blob 8f10e6ebff19 done  
Copying blob 486383b07939 done  
Copying blob 23be1053bf93 done  
Copying blob ee738432d587 done  
Copying config 60dde8abf7 done  
Writing manifest to image destination
Storing signatures
----
+
. Before we attempt to run our image, let's setup a log watch so that we can see the logs of the pod.  We'll do this in a separate terminal, so first split your terminal, this will be referred to as *Terminal 2*
+
image::terminal-split.jpg[]
+
[tabs]
====
Terminal 2::
+
--
To monitor our pod from the terminal we're going to use a tool called `stern` (see here for more info).  It allows us to aggregate and view logs from our Kubernetes cluster.

To set up a log watch enter the following command

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
stern -n {USER} \#<.>
   {ocp_insecure_pod_name} #<.>
----
<.> The `-n` option tells the stern to watch for pods in the `{USER}` namespace
<.> This is a string that stern uses to match against any pods that exist in the specified namespace.  In this case this matches the name we are intending to give our pod
--
====
+
. With our image pushed to the OpenShift image registry, we can now run the image in a pod using the following command (and the "Cluster Local" name of the registry which we obtained above)
+
[tabs]
====
Terminal 1::
+
--
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
oc run \
    {ocp_insecure_pod_name}\#<.>
    -n {OCP_PROJECT} \
    --image pass:[${CLUSTER_LOCAL_REGISTRY_URL}]:insecure #<.>
----
<.> Different pod name to distinguish from previous
<.> Same exact image registry, just different tag
--
====
+
. As the pod starts up we should see the following output from the `stern` watch in the other terminal
+
[tabs]
====
Terminal 2::
+
--
.Recurring `stern` output
[.console-output]
[source,bash,subs="+macros,+attributes,+quotes"]
----
+ {ocp_insecure_pod_name} › {ocp_insecure_pod_name} #<.>
Error opening stream to student1/{ocp_insecure_pod_name}: {ocp_insecure_pod_name}
: container "{ocp_insecure_pod_name}" in pod "{ocp_insecure_pod_name}" is waiting to start: ContainerCreating
- {ocp_insecure_pod_name}
+ {ocp_insecure_pod_name} › {ocp_insecure_pod_name}
Error opening stream to student1/{ocp_insecure_pod_name}: {ocp_insecure_pod_name}
: container "{ocp_insecure_pod_name}" in pod "{ocp_insecure_pod_name}" is waiting to start: ContainerCreating
- {ocp_insecure_pod_name}
+ {ocp_insecure_pod_name} › {ocp_insecure_pod_name}
{ocp_insecure_pod_name} {ocp_insecure_pod_name} AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 10.128.2.16. Set the 'ServerName' directive globally to suppress this message 
#{ocp_insecure_pod_name} {ocp_insecure_pod_name} (13)Permission denied: AH00058: Error retrieving pid file logs/httpd.pid#
#{ocp_insecure_pod_name} {ocp_insecure_pod_name} AH00059: Remove it before continuing if it is corrupted.#
- {ocp_insecure_pod_name}
---- 
<.> `stern` outputs logs in the format pod-name > container-name '
--
====
